import{_ as a,c as l,a as t,o as i}from"./app-DnNDZACx.js";const p={};function s(n,e){return i(),l("div",null,e[0]||(e[0]=[t('<h2 id="clap4clip" tabindex="-1"><a class="header-anchor" href="#clap4clip"><span>CLAP4CLIP</span></a></h2><h3 id="问题" tabindex="-1"><a class="header-anchor" href="#问题"><span>问题</span></a></h3><ul><li><p>多模态持续学习可能会出现不同模态之间的特征相互偏离，从这个角度上产生灾难性遗忘（通过实验证明会随着新任务的学习text模态和预训练得到的image模态特征之间的角度偏离越来越大）</p></li><li><p>确定性的微调建模方式会对视觉、文本语义之间的相互作用造成限制</p></li><li><p>VPT方法过于依赖于domain的分布，没有很好的泛化性；通过学习手动prompts的分布的方法与条件prompt学习无法兼容</p><blockquote><p><strong>条件prompting</strong>：根据输入数据或上下文信息，动态调整prompt内容，使得prompt根据不同的输入或上下文进行自适应优化。</p><p><strong>软提示</strong>（Soft Prompts）：使用一组可学习的向量标记$ p = {p^1, p^2, ..., p^L}$，共享任务中的所有类别。</p></blockquote></li><li><p>采用Adapter微调CLIP，会产生跨模态偏差的问题。传统的线性adapter常用于将预定义的功能空间投影到特定输出上.</p></li><li><p>对于概率adapter，搜索任务特定adapter的残差参数$\\alpha$开销是巨大的，且对于实验设置非常敏感，甚至可能破坏预训练的表征而造成灾难性遗忘。因此本实验将$\\alpha$统一设置为1，直接省去寻找该超参数的花费</p></li></ul><h3 id="研究背景" tabindex="-1"><a class="header-anchor" href="#研究背景"><span>研究背景</span></a></h3><ul><li>CIL</li></ul><h3 id="keypoints" tabindex="-1"><a class="header-anchor" href="#keypoints"><span>keypoints</span></a></h3><ul><li>采取了现有的<strong>概率微调</strong>方式，试图更好地捕捉跨模态之间的相互作用（in持续学习）。</li><li>采用herd，一个基于排练的方法进行重播，防止遗忘</li><li>发现文本编码器输出的特征空间更适合定义功能空间的先验。</li><li>该方法中文本特征依赖于训练学习，而视觉特征依赖于冻结的CLIP预训练模型得到</li></ul><h3 id="methods" tabindex="-1"><a class="header-anchor" href="#methods"><span>Methods</span></a></h3><p>提出了一个基于贝叶斯变分推断框架的概率微调模型，它基于功能空间，用于对齐特定任务的文本特征。</p><blockquote><p><strong>贝叶斯变分推理</strong>：选择一个参数化分布 $q(\\theta; \\phi)$，通过优化其参数$\\phi$使其尽量接近真实的后验分布 $p(\\theta | \\mathcal{D})$，从而将计算问题转换为好操作的优化问题。 优化目标为其ELBO：ELBO=Eq(θ)[logp(D∣θ)]−DKL(q(θ)∥p(θ))，力求将最大化 EBLO分为两部分：对数据求其对数的期望值，用于拟合观测数据；q(θ)和先验分布p(θ)的KL散度，用于约束分布的形状。 逐步更新q(θ)。</p></blockquote><h4 id="贝叶斯变分推理" tabindex="-1"><a class="header-anchor" href="#贝叶斯变分推理"><span>贝叶斯变分推理</span></a></h4><p>对于类别c的预测概率计算公式为：</p><p>此处将文本特征进行变形：将 $t_c(p)$ 表示为潜变量 $z$ 和基础文本嵌入的组合后，变得更加灵活。</p><p>再引入变分分布$q_\\phi(z | t_c)$，同样优化ELBO</p><p>最后得到变分分布，近似了后验分布。</p><h5 id="优势" tabindex="-1"><a class="header-anchor" href="#优势"><span>优势</span></a></h5><ul><li>相比起静态prompt方法，g(tc(p))更容易提取，因此文本特征生成更具有鲁棒性。</li><li>同时在嵌入中引入了随机性</li><li>有助于泛化</li></ul><h4 id="模态对齐" tabindex="-1"><a class="header-anchor" href="#模态对齐"><span>模态对齐</span></a></h4><p>提出了**视觉引导注意力(VGA)**模块进行跨模态特征对齐。</p><p>该方法利用视觉特征去引导文本特征，减小不同模态之间的偏差。</p><p>VGA利用了注意力机制，将之前得到文本特征作为Q（查询），同时学习一个显示注意力图（即视觉特征），作为K与V。具体实现用的是Transformer的解码器。 它首先执行文本的自注意力，在进行文本-视觉交叉注意力计算。通过该方法进行不同模态的对齐，利用视觉特征作为上下文指导文本特征的学习。</p><p>任务之间共享同一个VGA模块，利用目标掩码进行任务之间的隔离。</p><p>公式即为：</p><p>最后通过残差连接将视觉引导后得到的特征和原特征进行融合，得到最终的文本嵌入。</p><h4 id="moe-adapters" tabindex="-1"><a class="header-anchor" href="#moe-adapters"><span>MoE adapters</span></a></h4><p>此处的adapter也采取了MoE结构，（估计是k=1）。为每个task配备了一个专有的adapter,以及一个任务特定的蒙特卡洛样本$z^t_m$（？）。</p><ul><li><p>为了防止adapter的灾难性遗忘，在新任务训练时会对旧adapter进行冻结，同时维护一共混合数据集，在每次任务结束后在该数据集上进行微调（被称为巩固训练）</p></li><li><p>除此之外，利用hand-crafted特征蒸馏旧任务的潜特征$z^t$。具体做法为计算出$z^t$属于每个类的概率，然后计算交叉熵损失记为KD loss，进行正交约束，使得旧任务的潜在特征更接近于hand-crafted特征。该约束只发生在巩固训练阶段，因为此时旧任务的adapters是可训练的（？）。该步的目标是为了让每个任务的输出分布能够正交。</p></li><li><p>另外，还用之前任务的文本特征的均值和标准差对adapter进行初始化。</p></li></ul><h3 id="读后感" tabindex="-1"><a class="header-anchor" href="#读后感"><span>读后感</span></a></h3><ul><li>从消融实验看，VGA对性能提升起到主要作用，说明了模态对齐的重要性；其次是task-specific adapters。而初始化和正交约束的改进较小</li><li>采用了很多种方法来防止灾难性遗忘</li><li>由于CLIP是通过对比学习对齐的，也需要利用这一特性进行改进。</li></ul><h2 id="mma" tabindex="-1"><a class="header-anchor" href="#mma"><span>MMA</span></a></h2><h3 id="问题-1" tabindex="-1"><a class="header-anchor" href="#问题-1"><span>问题</span></a></h3><ul><li>CLIP的一些传统研究忽视了文本与视觉两个模态之间交互关系。</li><li>CIL</li></ul><h3 id="keypoints-1" tabindex="-1"><a class="header-anchor" href="#keypoints-1"><span>Keypoints</span></a></h3><ul><li><p>使用Adapter进行多模态融合</p></li><li><p>主要提高了在unseen class上的泛化性</p></li><li><p>在embedding环境用adapter融合了跨模态特征，然后再与单独的embedding进行结合，得到adapted embedding（这步其实也是在执行模态之间相互引导生成embedding的过程）</p></li></ul><h3 id="methods-1" tabindex="-1"><a class="header-anchor" href="#methods-1"><span>Methods</span></a></h3><p>MM Adapter采用了多头注意力网络架构。MMA包括了三部分：1）嵌入下采样。2）掩码多头注意力网络。3）包含非线性激活函数的两层线性层</p><h4 id="嵌入下采样" tabindex="-1"><a class="header-anchor" href="#嵌入下采样"><span>嵌入下采样</span></a></h4><p>其主要目的是为了减少多头注意力部分的计算代价。它由线性层组成。</p><h4 id="掩码多头注意力" tabindex="-1"><a class="header-anchor" href="#掩码多头注意力"><span>掩码多头注意力</span></a></h4><p>通过掩码区分两个模态，同时激发模态内的交互。它在计算掩码的时候只保留正向交互部分，即属于不同模态时，允许交互，避免模态内的冗余信息影响。通过设计掩码，确保每个模态的嵌入主要基于另一模态的信息进行更新，从而实现更好的跨模态融合效果。</p><h4 id="mma-1" tabindex="-1"><a class="header-anchor" href="#mma-1"><span>MMA</span></a></h4><p>adapter由两个线性层和非线性激活函数组成。adapter的输出被拆分为adapted文本嵌入和adapted视觉嵌入，然后再进行标准化，再与原embedding向量进行相加</p><h3 id="code" tabindex="-1"><a class="header-anchor" href="#code"><span>code</span></a></h3><p>使用mlp作为adapter，将原始text和image特征拼接在一起分别送到两个adapter中得到adapted特征，然后再跟原始特征进行加权融合，得到了最终融合后的text和image特征向量。</p><p>除此之外还提供了多种多模态融合的方法，包括MaskedMultiHeadAttention、带有降采样的MaskedMultiHeadAttention、降维+mmha adapter+升维、单adapter等</p><h3 id="读后感-1" tabindex="-1"><a class="header-anchor" href="#读后感-1"><span>读后感</span></a></h3><ul><li>是否说明adapter对性能提升效果不大？</li><li>为什么训练特定于任务的adapters反而能提升泛化性能呢？</li><li>这篇主要提供了一种多模态融合的方法，也采用了分类作为问题背景。代码写法封装性很强，规范性很强。但它不涉及到持续学习方面。因此我认为可以在其它代码框架中可以采用它的多模态融合方法。</li><li>它是用adapter进行多模态融合，有用MLP、MMHA作为adapter（以及一些优化措施）。我们可以也采用adapter用以特征融合，但是在shared_adapter中还是开辟另外的特征融合模块呢？</li></ul><h2 id="vqacl" tabindex="-1"><a class="header-anchor" href="#vqacl"><span>VQACL</span></a></h2><h3 id="背景" tabindex="-1"><a class="header-anchor" href="#背景"><span>背景</span></a></h3><blockquote><p><strong>VQA</strong>: 视觉问答。目标是根据给定的图片和与图片相关的问题，生成正确的答案。 V:图像 Q：问题 A:回答</p></blockquote><p>与图片分类问题不同，该方法的问题领域为视觉问答。</p><p>算是PIL?</p><h3 id="keypoints-2" tabindex="-1"><a class="header-anchor" href="#keypoints-2"><span>Keypoints</span></a></h3><p>也依靠了随机重播的方式减小遗忘</p><p>该方法主要分为两个模块：用于对多模态信息建模；提高持续学习的泛化性。前者的模块叫做视觉文本数据嵌套的双级任务序列，后者叫做包含 新视觉概念 的 novel skill-concept compostition。</p><h3 id="methods-2" tabindex="-1"><a class="header-anchor" href="#methods-2"><span>Methods</span></a></h3><h4 id="双级任务序列" tabindex="-1"><a class="header-anchor" href="#双级任务序列"><span>双级任务序列</span></a></h4><p>将任务分为outer level和inner level。</p><p>outer level 用于处理文本逻辑，即问题的类型，如“xx是什么颜色”“有几个xx?”</p><p>inner level首先处理图像的物体类别， 然后根据对象类别进一步构建了一个随机排序的子任务序列，如“叶子是什么颜色”“手机是什么颜色”</p><p>因为双级的引入以及VQA本身的特性，在给VQACL的稳定性-可塑性以外，还添加上了组合性，即任务和对象的组合。</p><h4 id="novel-skill-concept-composition" tabindex="-1"><a class="header-anchor" href="#novel-skill-concept-composition"><span>novel skill-concept composition</span></a></h4><p>对outer level上的每个任务随机mask一个它的子任务，并将该子任务用于训练时的测试，用于提高泛化性。</p><h4 id="ss和si" tabindex="-1"><a class="header-anchor" href="#ss和si"><span>SS和SI</span></a></h4><p>它将样本的特征分为<strong>样本特异性（SS）<strong>和</strong>样本不变量（SI）</strong>,这两个特征分别学习得到。</p><p>SS特征通过多个transformer encoder获得，得到每个样本最突出的个性特征</p><p>SI特征通过prototype学习得到，即聚类的方法获得类型学习，该特征具有稳定性和代表性，能实现很好的泛化性。</p><h4 id="decoder" tabindex="-1"><a class="header-anchor" href="#decoder"><span>Decoder</span></a></h4><p>采用了多层transformer decoder层堆叠而成，每个块都有一个额外的交叉注意力层用于多模态融合</p><h3 id="code-1" tabindex="-1"><a class="header-anchor" href="#code-1"><span>code</span></a></h3><p>维持了一个记忆池存放历史任务中的类别（同时进行动态平衡），然后通过replay进行防止灾难性遗忘</p><p>使用了V和Q的prototype，指的是每个任务的文本和视觉平均表示（用于代表任务？）每次任务结束后进行更新，为历史prototype和当前prototype的加权和</p><h3 id="读后感-2" tabindex="-1"><a class="header-anchor" href="#读后感-2"><span>读后感</span></a></h3><ul><li>这种双级任务难道是通过学习更为底层的逻辑来防止灾难性以往的吗？（有点难理解）</li><li>感觉防止遗忘的手段有点弱</li><li>感觉大多数手段都是针对VQA这个问题领域特点的，不知道有无推广价值</li><li>并没有用到adapter，算是直接集成到encoder和decoder中了</li><li>本项目代码的问题-目标分离属性比较明显，不知道是否要延续还是另起炉灶（track这类问题有意义吗）</li><li>感觉VQA问题是不是还是在于VQ对齐。需要想出一个适配VQA这个问题背景的有说服力的思路</li></ul><h2 id="triplet" tabindex="-1"><a class="header-anchor" href="#triplet"><span>TRIPLET</span></a></h2><h3 id="背景-1" tabindex="-1"><a class="header-anchor" href="#背景-1"><span>背景</span></a></h3><p>依然是CL-VQA问题</p><h3 id="keypoints-3" tabindex="-1"><a class="header-anchor" href="#keypoints-3"><span>Keypoints</span></a></h3><p>利用预训练模型参数进行初始化</p><h3 id="formulation" tabindex="-1"><a class="header-anchor" href="#formulation"><span>Formulation</span></a></h3><p>对VQA-CL问题进行了规范化，它将其拆分成三个子问题:</p><ul><li>视觉分布发生变化，而问题不变</li><li>问题文本分布发生变化，视觉场景不变</li><li>视觉分布与问题文本同时发生变化</li></ul><h3 id="methods-3" tabindex="-1"><a class="header-anchor" href="#methods-3"><span>Methods</span></a></h3><h4 id="prompt-decoupling" tabindex="-1"><a class="header-anchor" href="#prompt-decoupling"><span>Prompt Decoupling</span></a></h4><p>它通过附加prompt的方式进行解耦。给视觉V和问题Q分别都添加了各自prompt，还添加了融合的prompt。</p><p>但它并不是对每层进行解耦，而是选择特定层添加prompt。该策略通过减少无关层的干扰，使得prompt在需要的地方更高效地发挥作用。</p><p>就如VQACL中将特征划分为SS和SI，它也将prompt分为量类：G-prompt和E-Prompt。前者用于解耦通用知识，后者用于解耦任务特定知识。</p><h4 id="任务识别" tabindex="-1"><a class="header-anchor" href="#任务识别"><span>任务识别</span></a></h4><p>提出了一tjhg个叫Query-and-Match Strategy的查询策略，它通过计算模态融合后的特征和任务的密钥（可看作为每个任务生成一个标识符）的余弦相似度。同时也有一个查询损失函数用于提高任务识别的清晰度。</p><h4 id="模态融合" tabindex="-1"><a class="header-anchor" href="#模态融合"><span>模态融合</span></a></h4><p>使用矩阵乘法进行融合，公式如下： $$ P_{t,k}^{(f)} = W_k^{(v)} \\otimes P_{t,k}^{(v)} + W_k^{(q)} \\otimes P_{t,k}^{(q)}</p><ul><li>W_k^{(v \\otimes q)} \\otimes \\left( P_{t,k}^{(v)} \\otimes P_{t,k}^{(q)} \\right)</li><li>W_k^{(q \\otimes v)} \\otimes \\left( P_{t,k}^{(q)} \\otimes P_{t,k}^{(v)} \\right) $$ 同时还对这些$W$矩阵进行了低秩约束</li></ul><p>使用$L_{mod}$解决模态交互（？没太理解） $$ L_{mod}(D_t) = -\\sum_k \\gamma \\left( P_{t,k}^{(f)}, P_{t,k}^{(l)} \\right) $$</p><h4 id="任务间融合" tabindex="-1"><a class="header-anchor" href="#任务间融合"><span>任务间融合</span></a></h4><p>不同任务的prompts共享同一个语义空间，<strong>任务交互策略</strong> 旨在优化任务特定提示之间的交互效果（这里也没看懂？）<strong>任务交互损失</strong>（LtaskL_{task}Ltask）是通过约束提示矩阵 W(a)W^{(a)}W(a) 的相互作用来引导模型的训练，确保提示之间的协同作用符合任务需求。</p><h3 id="读后感-3" tabindex="-1"><a class="header-anchor" href="#读后感-3"><span>读后感</span></a></h3><ul><li>模态融合方法是否太弱了</li><li>怎么防止灾难性遗忘？（但指标表现还挺好的）</li></ul><h2 id="ctp" tabindex="-1"><a class="header-anchor" href="#ctp"><span>CTP</span></a></h2><p><strong>Compatible Momentum Contrast and Topology Preservation</strong>：可兼容的动量对比与拓扑保持</p><h3 id="背景-2" tabindex="-1"><a class="header-anchor" href="#背景-2"><span>背景</span></a></h3><p>视觉语言持续预训练（VLCP）</p><h3 id="问题-2" tabindex="-1"><a class="header-anchor" href="#问题-2"><span>问题</span></a></h3><ul><li>传统大多数持续学习方法都使用固定的共享单元维度来融合新旧知识</li><li>在新旧任务融合时，由于旧任务对比样本的缺失，会导致共享单元的性能不平衡</li></ul><h3 id="methods-4" tabindex="-1"><a class="header-anchor" href="#methods-4"><span>Methods</span></a></h3><blockquote><p><strong>动量模型</strong>是通过动量更新机制（Momentum Update）维护的一个模型版本，通常用于提供一个更稳定的特征或梯度监督信号。它通常作为 <strong>教师模型</strong> 或 <strong>目标模型</strong>。动量模型的参数更新非常缓慢，捕捉了主模型（学生模型）的长期趋势。</p><p><strong>参考模型（Reference Model）</strong> 通常是指一个已经训练好的模型，用于作为比较或初始化当前模型的基础。</p><p>该方法维护了一个动量模型与参考模型，估计是用动量模型来捕捉任务间的共性长期特征，参考模型用于新旧任务约束，防止灾难性遗忘。在第一个任务的时候利用主模型来更新动量模型，而在之后的任务使用参考模型和主模型共同来更新动量模型。</p></blockquote><h4 id="兼容动量" tabindex="-1"><a class="header-anchor" href="#兼容动量"><span>兼容动量</span></a></h4><p>用于吸收和融合新旧知识，并调整单模态、多模态的encoders。该模块负责了模态特征编码和融合。</p><p>使用ViT作为图像编码器，BERT的前六层作为文本编码器，BERT的后六层作为多模态编码器。提取出来的文本特征和图像特征在多模态编码器中通过交叉注意力进行融合，同时也使用了两个线性矩阵将文本特征和图像特征映射到低维空间进行对齐。 除此之外，该模型还会计算文本-图像对的余弦相似度，并建立相似度矩阵，然后计算该矩阵的对称交叉熵$\\mathcal{L}<em>{ita}^c=\\frac{\\mathcal{L}</em>{i2t}+\\mathcal{L}<em>{t2i}}{2}$来优化对齐效果。 另外也通过掩码模型来增强文本和图像的融合。通过随机mask token的方式强制模型根据文本上下文和图像信息来推理该mask部分，加强多模态理解整合能力。该优化损失函数为 $$ L</em>{mlm}^c = -\\mathbb{E}<em>{(I, \\hat{T}) \\sim D_t} \\mathcal{H}(y^m, p</em>\\theta^m(I, \\hat{T})) $$ 因此兼容动量对比损失为$L_{CMC}=L^c_{ita}+L^c_{mlm}$</p><p>它还抑制了同模态最大相似度（只在消融实验中简单提到）</p><h4 id="拓扑保持" tabindex="-1"><a class="header-anchor" href="#拓扑保持"><span>拓扑保持</span></a></h4><p>该部分主要用于处理任务间信息传递与防止灾难性遗忘。</p><p>它基于的假设是训练新任务前后的模型的image2text和text2image相似度预测分布应该是相似的，因此用这点来约束持续学习模型的拓扑结构的一致性。 该实现方法为计算新任务前后的相似度矩阵的交叉熵作为损失函数： $$ L_c = \\frac{1}{2} \\mathbb{E}<em>{(I, T) \\sim D_t} \\big[ H(\\hat{p}^{i2t}</em>{t-1}, \\hat{p}^{i2t}<em>{\\theta_t}) + H(\\hat{p}^{t2i}</em>{t-1}, \\hat{p}^{t2i}<em>{\\theta_t}) \\big] \\tag{9} $$ 同时为了防止同样本对的相似性过大，对其他样本对造成不公平的蒸馏影响，便将相似度矩阵对角线的值修改为极小值，使模型关注更多样本的相对关系。同样利用损失函数来实现： $$ L_s = \\frac{1}{2} \\mathbb{E}</em>{(I, T) \\sim D_t} \\big[ H(\\hat{p}^{i2i}<em>{t-1}, \\hat{p}^{i2i}</em>{\\theta_t}) + H(\\hat{p}^{t2t}<em>{t-1}, \\hat{p}^{t2t}</em>{\\theta_t}) \\big] \\tag{9} $$</p><h3 id="code-2" tabindex="-1"><a class="header-anchor" href="#code-2"><span>code</span></a></h3><p>任务间并没有特意的代码块来进行处理。而是仅通过计算loss_dis（辅助ref_model）来进行约束，防止灾难性遗忘，除此之外并无更多针对不同任务的措施。</p><p>loss_mlm_dis是当前模型和ref_model之间输出的mlm_logits之间的蒸馏损失（估计是从mlm能力的角度来约束新旧任务，防止遗忘）</p><p>loss_mlm除了模型中loss_mlm的计算，还加上了model和动量模型之间的蒸馏损失（估计用于动量模型约束）</p><p>在后续的任务中，还会对动量矩阵进行mlm和ita约束，即loss_mlm_dis与loss_ita_dis</p><h3 id="想法" tabindex="-1"><a class="header-anchor" href="#想法"><span>想法</span></a></h3><p>这个代码乱乱的。。而且到现在也不知道CLIP在哪，以及持续预训练是个什么东西。python代码版本也很低，总之就是这个代码的重构性很差。</p><p>但我认为针对这个方法本身是有提升空间的，这个方法本身较为一般，而且VLCL这个问题领域涉及的也很少，就是可能得自己制定架构。（不知道这篇投中跟它数据集的贡献有多少）。</p><p>还有一个就是数据集的问题。</p><h3 id="读后感-4" tabindex="-1"><a class="header-anchor" href="#读后感-4"><span>读后感</span></a></h3><ul><li>视觉语言和VQA有区别吗</li><li>持续学习和持续预训练有区别吗</li><li>文章中说通过masked modeling的多模态融合特征具有较强的抗遗忘能力，而缺乏不同任务样本间对比的方法容易遗忘（多模态融合比跨模态对齐具有更强的抗遗忘能力）。这是否说明了任务间的对比学习的重要性？</li><li>通过mask加强模态融合的部分是否可以通过强制手段来调整根据图像信息推测和根据上下文文本推测的比重？（不做任何约束是否会出现主要还是靠文本上下文推理的情况？）</li><li>在单模态上效果一般，感觉改进点主要还是在模态融合上</li><li>在对抗灾难性遗忘上有改进空间。</li><li>过大的拓扑保持约束会影响新任务的可塑性？对比其他方法少了对task-specific的特征提取</li><li>对VLCL任务来说无需类别先验（如聚类）也是比较重要的</li><li>VLCL这个领域是否利好shared_adapter呢？（相比起TIL也许会有更多的共性信息）</li></ul><h2 id="一些想法" tabindex="-1"><a class="header-anchor" href="#一些想法"><span>一些想法</span></a></h2><ul><li><p>shared adapter能否用于模态之间共享信息？</p></li><li><p>如何对齐跨模态特征、如何对齐不同任务的跨模态特征是问题关键（是否能在对齐模块注入CIL的特征？），要提供给模型合适的包含多模态信息的视野</p></li><li><p>对于问题领域应该怎么抉择</p></li><li><p>是否可以认为文本特征主要侧重它的推理能力，而图像特征侧重于信息提取能力</p></li><li><p>这些文章的一部分的亮点就在于能提高泛化性、</p></li><li><p>这些文章用到的任务数相对都较少，这可能也受到了数据集的限制</p></li><li><p>可解释性？</p></li><li><p>该类问题的主要两个Point:模态间融合与任务间融合（灾难性遗忘和泛化性）</p></li><li><p>CTP所指出的问题也是shared-adapter需要解决的问题</p></li><li><p>存储内存也是一个重要的point</p></li><li><p>VLCL，VQACL这些概念很散，需不需要将它们重新定义（这也涉及到formulation，实验set和数据集的问题）</p></li></ul>',125)]))}const r=a(p,[["render",s],["__file","0103.html.vue"]]),d=JSON.parse('{"path":"/blogs/CL-vlm/0103.html","title":"first paper","lang":"en-US","frontmatter":{"title":"first paper","date":"2025/01/03","tags":["CIL","VQA","VL"],"categories":["CL-vlm"]},"headers":[{"level":2,"title":"CLAP4CLIP","slug":"clap4clip","link":"#clap4clip","children":[{"level":3,"title":"问题","slug":"问题","link":"#问题","children":[]},{"level":3,"title":"研究背景","slug":"研究背景","link":"#研究背景","children":[]},{"level":3,"title":"keypoints","slug":"keypoints","link":"#keypoints","children":[]},{"level":3,"title":"Methods","slug":"methods","link":"#methods","children":[]},{"level":3,"title":"读后感","slug":"读后感","link":"#读后感","children":[]}]},{"level":2,"title":"MMA","slug":"mma","link":"#mma","children":[{"level":3,"title":"问题","slug":"问题-1","link":"#问题-1","children":[]},{"level":3,"title":"Keypoints","slug":"keypoints-1","link":"#keypoints-1","children":[]},{"level":3,"title":"Methods","slug":"methods-1","link":"#methods-1","children":[]},{"level":3,"title":"code","slug":"code","link":"#code","children":[]},{"level":3,"title":"读后感","slug":"读后感-1","link":"#读后感-1","children":[]}]},{"level":2,"title":"VQACL","slug":"vqacl","link":"#vqacl","children":[{"level":3,"title":"背景","slug":"背景","link":"#背景","children":[]},{"level":3,"title":"Keypoints","slug":"keypoints-2","link":"#keypoints-2","children":[]},{"level":3,"title":"Methods","slug":"methods-2","link":"#methods-2","children":[]},{"level":3,"title":"code","slug":"code-1","link":"#code-1","children":[]},{"level":3,"title":"读后感","slug":"读后感-2","link":"#读后感-2","children":[]}]},{"level":2,"title":"TRIPLET","slug":"triplet","link":"#triplet","children":[{"level":3,"title":"背景","slug":"背景-1","link":"#背景-1","children":[]},{"level":3,"title":"Keypoints","slug":"keypoints-3","link":"#keypoints-3","children":[]},{"level":3,"title":"Formulation","slug":"formulation","link":"#formulation","children":[]},{"level":3,"title":"Methods","slug":"methods-3","link":"#methods-3","children":[]},{"level":3,"title":"读后感","slug":"读后感-3","link":"#读后感-3","children":[]}]},{"level":2,"title":"CTP","slug":"ctp","link":"#ctp","children":[{"level":3,"title":"背景","slug":"背景-2","link":"#背景-2","children":[]},{"level":3,"title":"问题","slug":"问题-2","link":"#问题-2","children":[]},{"level":3,"title":"Methods","slug":"methods-4","link":"#methods-4","children":[]},{"level":3,"title":"code","slug":"code-2","link":"#code-2","children":[]},{"level":3,"title":"想法","slug":"想法","link":"#想法","children":[]},{"level":3,"title":"读后感","slug":"读后感-4","link":"#读后感-4","children":[]}]},{"level":2,"title":"一些想法","slug":"一些想法","link":"#一些想法","children":[]}],"git":{"createdTime":1737880610000,"updatedTime":1737880610000,"contributors":[{"name":"lluosi","email":"1340498501@qq.com","commits":1}]},"filePathRelative":"blogs/CL-vlm/0103.md"}');export{r as comp,d as data};
